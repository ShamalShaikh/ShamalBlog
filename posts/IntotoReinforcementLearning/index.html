<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Introduction to Reinforcement Learning" /><meta property="og:locale" content="en" /><meta name="description" content="Multi-Armed Bandits in Reinforcement Learning Course (3 Credits)" /><meta property="og:description" content="Multi-Armed Bandits in Reinforcement Learning Course (3 Credits)" /><link rel="canonical" href="https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/" /><meta property="og:url" content="https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/" /><meta property="og:site_name" content="Shamal Shaikh" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-03T21:30:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Introduction to Reinforcement Learning" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-01-03T21:30:00+05:30","datePublished":"2022-01-03T21:30:00+05:30","description":"Multi-Armed Bandits in Reinforcement Learning Course (3 Credits)","headline":"Introduction to Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/"},"url":"https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/"}</script><title>Introduction to Reinforcement Learning | Shamal Shaikh</title><link rel="apple-touch-icon" sizes="180x180" href="/ShamalBlog/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/ShamalBlog/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/ShamalBlog/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/ShamalBlog/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/ShamalBlog/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shamal Shaikh"><meta name="application-name" content="Shamal Shaikh"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/ShamalBlog/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/ShamalBlog/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/ShamalBlog/" alt="avatar" class="mx-auto"> <img src="/ShamalBlog/shamal-suit3.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/ShamalBlog/">Shamal Shaikh</a></div><div class="site-subtitle font-italic">knowledge shared is knowledge squared</div></div><ul class="w-100"><li class="nav-item"> <a href="/ShamalBlog/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/ShamalBlog/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/ShamalBlog/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/ShamalBlog/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/ShamalBlog/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/ShamalBlog/projects/" class="nav-link"> <i class="fa-fw fas fa-project-diagram ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/ShamalBlog/writing/" class="nav-link"> <i class="fa-fw fas fa-pen-fancy ml-xl-3 mr-xl-3 unloaded"></i> <span>TECHNICAL WRITING</span> </a><li class="nav-item"> <a href="/ShamalBlog/skills/" class="nav-link"> <i class="fa-fw fas fa-chart-bar ml-xl-3 mr-xl-3 unloaded"></i> <span>SKILLS & EXPERTISE</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/ShamalShaikh" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/shamal-shaikh" aria-label="linkedin" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shamalshaikh','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/ShamalBlog/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG â€º <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/ShamalBlog/"> Home </a> </span> <span>Introduction to Reinforcement Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Introduction to Reinforcement Learning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Shamal Shaikh </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 3, 2022, 9:30 PM +0530" >Jan 3, 2022<i class="unloaded">2022-01-03T21:30:00+05:30</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="714 words">3 min read</span></div></div><div class="post-content"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1000 400'%3E%3C/svg%3E" data-proofer-ignore data-src="/ShamalBlog/images/MC/5G.gif" class="preview-img" alt="k-armed-bandit" width="1000" height="400"><h2 id="multi-armed-bandits-in-reinforcement-learning-course-3-credits">Multi-Armed Bandits in Reinforcement Learning Course (3 Credits)</h2><h3 id="1-introduction-to-reinforcement-learning">1. Introduction to Reinforcement Learning</h3><h3 id="2-multi-armed-bandit-problem">2. Multi-Armed Bandit Problem</h3><p><strong>2.1 Ak-armed Bandit Problem</strong><br /> <strong>2.2 Action-value Methods</strong><br /> <strong>2.3 The 10-armed Testbed</strong><br /> <strong>2.4 Incremental Implementation</strong><br /> <strong>2.5 Tracking a Nonstationary Problem</strong><br /> <strong>2.6 Optimistic Initial Values</strong><br /> <strong>2.7 Upper-Confidence-Bound Action Selection</strong><br /> <strong>2.8 Gradient Bandit Algorithms</strong><br /> <strong>2.9 Associative Search (Contextual Bandits)</strong><br /> <strong>2.10 Summary</strong></p><hr /><p>A Good start to the topic of our discussion - <strong>Reinforcement Learning and Multi-Armed Bandit Problem</strong>.</p><p>Ask yourself:</p><ol><li>How can we make optimal decisions when faced with multiple uncertain choices?<li>How do we balance exploring new options and exploiting known rewards?</ol><p>To dive into these questions, we first explore the characteristics of multi-armed bandits:</p><ul><li><p><strong>Ak-armed Bandit Problem</strong>: This foundational concept in reinforcement learning involves a scenario where we have multiple actions (arms) to choose from, each providing a different, unknown reward. The goal is to maximize the total reward over time by learning which actions are the best. This problem models various real-world scenarios, such as clinical trials, financial investments, and adaptive advertising strategies.</p><li><p><strong>Action-value Methods</strong>: These methods help estimate the expected reward of actions to guide decision-making. By maintaining and updating value estimates for each action, we can balance exploration (trying new actions) and exploitation (choosing the best-known action). Key techniques include:</p><ul><li><strong>Sample-average method</strong>: Updates the value estimate with the average of observed rewards.<li><strong>Weighted average method</strong>: Gives more weight to recent observations, useful in nonstationary environments.</ul></ul><p><img data-proofer-ignore data-src="https://example.com/action-value-methods.png" alt="Action-Value Methods" /> <em>action-value methods</em></p><ul><li><p><strong>The 10-armed Testbed</strong>: A standard experimental setup to evaluate and compare different action-value methods. This testbed involves a simulated environment with ten arms, each providing rewards from a normal distribution with different means. It helps understand the effectiveness of various strategies under controlled conditions, providing insights into their strengths and weaknesses.</p><li><p><strong>Incremental Implementation</strong>: This refers to updating action-value estimates efficiently without needing to store all past rewards. The key formula for incremental updates is: [ Q_{n+1} = Q_n + \frac{1}{n} \left( R_n - Q_n \right) ] This approach ensures updates are computationally inexpensive and memory efficient, making it suitable for real-time applications.</p><li><p><strong>Tracking a Nonstationary Problem</strong>: In nonstationary environments, the reward probabilities of actions change over time. To adapt, we use methods like:</p><ul><li><strong>Exponential recency-weighted average</strong>: Gives more weight to recent rewards, quickly adapting to changes.<li><strong>Sliding window average</strong>: Considers only the most recent observations, discarding old data.</ul></ul><p><img data-proofer-ignore data-src="https://example.com/nonstationary-problems.png" alt="Tracking Nonstationary Problems" /> <em>tracking nonstationary problems</em></p><ul><li><p><strong>Optimistic Initial Values</strong>: This technique involves starting with high value estimates for all actions, encouraging exploration since initial trials are likely to be rewarded, leading to quicker discovery of high-reward actions.</p><li><p><strong>Upper-Confidence-Bound Action Selection</strong>: This method balances exploration and exploitation by considering both the estimated value and the uncertainty of each action. The UCB formula is: [ A_t = \arg \max_a \left( Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right) ] Here, ( Q_t(a) ) is the estimated value, ( N_t(a) ) is the count of action ( a ) being chosen, and ( c ) is a confidence level parameter. UCB ensures that actions with fewer trials are more likely to be explored.</p><li><p><strong>Gradient Bandit Algorithms</strong>: These algorithms adjust the action preferences directly rather than estimating action values. They use a softmax distribution to choose actions and update preferences based on received rewards. The preference update rule is: [ P_{n+1}(a) = P_n(a) + \alpha (R_n - \bar{R})(1 - P_n(a)) ] where ( \alpha ) is a learning rate and ( \bar{R} ) is the average reward.</p></ul><p><img data-proofer-ignore data-src="https://example.com/gradient-bandit.png" alt="Gradient Bandit Algorithms" /> <em>gradient bandit algorithms</em></p><ul><li><strong>Associative Search (Contextual Bandits)</strong>: Contextual bandits extend the multi-armed bandit problem by incorporating context or state information into the decision-making process. Each armâ€™s reward is conditioned on the current context, allowing for more informed and tailored action selection. Applications include personalized recommendations and adaptive treatment strategies in healthcare.</ul><h3 id="210-summary">2.10 Summary</h3><p>The multi-armed bandit problem and its various methods provide a rich framework for understanding and solving real-world decision-making problems. From basic action-value methods to advanced techniques like UCB and contextual bandits, each approach offers unique insights and tools for tackling uncertainty and optimizing rewards.</p><hr /><p>By exploring these concepts, we gain a deeper appreciation for the challenges and solutions in reinforcement learning. Whether in academia or industry, mastering these techniques empowers us to build intelligent systems capable of making optimal decisions in uncertain environments.</p><p><img data-proofer-ignore data-src="https://example.com/summary.png" alt="Summary" /> <em>summary</em></p><p>I hope you liked it!</p><p>Wanna know more about me? Follow me on <a href="https://github.com/ShamalShaikh"><strong>GitHub</strong></a> and <a href="https://www.linkedin.com/in/shamal-shaikh/"><strong>LinkedIn</strong></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/ShamalBlog/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/ShamalBlog/categories/introduction/'>Introduction</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/ShamalBlog/tags/learning/" class="post-tag no-text-decoration" >learning</a> <a href="/ShamalBlog/tags/rl/" class="post-tag no-text-decoration" >RL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Introduction to Reinforcement Learning - Shamal Shaikh&url=https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/" data-toggle="tooltip" data-placement="top" title="LinkedIn" target="_blank" rel="noopener" aria-label="LinkedIn"> <i class="fa-fw fab fa-linkedin"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Introduction to Reinforcement Learning - Shamal Shaikh&u=https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Introduction to Reinforcement Learning - Shamal Shaikh&url=https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/ShamalBlog/posts/AI-Weekly-Digest/">AI Weekly Digest: Latest in LLMs and Open Source AI</a><li><a href="/ShamalBlog/posts/DeepSeek-Innovation/">DeepSeek's Game-Changing Approach to Large Language Models</a><li><a href="/ShamalBlog/posts/Understanding-AI-Agents/">Understanding AI Agents</a><li><a href="/ShamalBlog/posts/IntroToMobileComputing/">Introduction to Mobile Computing</a><li><a href="/ShamalBlog/posts/FreshStart/">Fresh Blogging Start</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/ShamalBlog/tags/llm/">llm</a> <a class="post-tag" href="/ShamalBlog/tags/python/">python</a> <a class="post-tag" href="/ShamalBlog/tags/fastapi/">fastapi</a> <a class="post-tag" href="/ShamalBlog/tags/ai/">ai</a> <a class="post-tag" href="/ShamalBlog/tags/learning/">learning</a> <a class="post-tag" href="/ShamalBlog/tags/agents/">agents</a> <a class="post-tag" href="/ShamalBlog/tags/ai-research/">ai-research</a> <a class="post-tag" href="/ShamalBlog/tags/ai-trends/">ai-trends</a> <a class="post-tag" href="/ShamalBlog/tags/api-design/">api-design</a> <a class="post-tag" href="/ShamalBlog/tags/automation/">automation</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/ShamalBlog/posts/IntroToMobileComputing/"><div class="card-body"> <span class="timeago small" >Jan 2, 2022<i class="unloaded">2022-01-02T21:30:00+05:30</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Introduction to Mobile Computing</h3><div class="text-muted small"><p> Mobile Computing Course (3 Credits) 1. Introduction to Mobile Networks 2. Types - MANET, WSN, VANET, PAN, DTN, Cellular 3. Reference Models for Network communication A Good start to the topic of o...</p></div></div></a></div><div class="card"> <a href="/ShamalBlog/posts/DeepSeek-Innovation/"><div class="card-body"> <span class="timeago small" >Feb 8<i class="unloaded">2025-02-08T12:00:00+05:30</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DeepSeek's Game-Changing Approach to Large Language Models</h3><div class="text-muted small"><p> The Rise of DeepSeek ðŸš€ In the rapidly evolving landscape of Large Language Models (LLMs), DeepSeek has emerged as a groundbreaking player, challenging the dominance of closed-source models. What m...</p></div></div></a></div><div class="card"> <a href="/ShamalBlog/posts/FastAPI-LLM-Integration/"><div class="card-body"> <span class="timeago small" >Feb 5<i class="unloaded">2025-02-05T12:00:00+05:30</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Production-Ready LLM Applications with FastAPI</h3><div class="text-muted small"><p> Building Scalable LLM Applications with FastAPI In this tutorial, Iâ€™ll show you how to build a production-ready LLM application using FastAPI, focusing on best practices and performance optimizati...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/ShamalBlog/posts/IntroToMobileComputing/" class="btn btn-outline-primary" prompt="Older"><p>Introduction to Mobile Computing</p></a> <a href="/ShamalBlog/posts/LinkedinAgent-Development/" class="btn btn-outline-primary" prompt="Newer"><p>LinkedinAgent: Automating Job Search with AI</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://shamalshaikh.github.io/ShamalBlog/posts/IntotoReinforcementLearning/'; this.page.identifier = '/posts/IntotoReinforcementLearning/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> Â© 2025 <a href="https://github.com/ShamalShaikh">Shamal Shaikh</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/ShamalBlog/tags/llm/">llm</a> <a class="post-tag" href="/ShamalBlog/tags/python/">python</a> <a class="post-tag" href="/ShamalBlog/tags/fastapi/">fastapi</a> <a class="post-tag" href="/ShamalBlog/tags/ai/">ai</a> <a class="post-tag" href="/ShamalBlog/tags/learning/">learning</a> <a class="post-tag" href="/ShamalBlog/tags/agents/">agents</a> <a class="post-tag" href="/ShamalBlog/tags/ai-research/">ai research</a> <a class="post-tag" href="/ShamalBlog/tags/ai-trends/">ai trends</a> <a class="post-tag" href="/ShamalBlog/tags/api-design/">api design</a> <a class="post-tag" href="/ShamalBlog/tags/automation/">automation</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/ShamalBlog/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/ShamalBlog/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/ShamalBlog/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8SLYBQTNNJ"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8SLYBQTNNJ'); }); </script>
